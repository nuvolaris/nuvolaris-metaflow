#
# This is an example of the command line generated buy the @nuvolaris decorator to launch the metaflow step annotated with @nuvolaris in a separate subprocess
#
/home/nuvolaris/.pyenv/versions/3.10.0/bin/python3 examples/helloworld.py --quiet --metadata local --environment local --datastore local --event-logger nullSidecarLogger --monitor nullSidecarMonitor --datastore-root /workspaces/nuvolaris/nuvolaris-controller/demo-metaflow/.metaflow nuvolaris step hello d9f0a1e311f6479feff618e2c66b46c175dc97bc /workspaces/nuvolaris/nuvolaris-controller/demo-metaflow/.metaflow/HelloFlow/data/d9/d9f0a1e311f6479feff618e2c66b46c175dc97bc --run-id 1669147318382288 --task-id 2 --input-paths 1669147318382288/start/1 --retry-count 0 --max-user-code-retries 0 --namespace user:timpefr --action mf --nuv-namespace metaflow --run-time-limit 432000


#
# This is an example of the command line that should be executed by the deployed nuvolaris metaflow action
#
['bash', '-c', '${METAFLOW_INIT_SCRIPT:+eval \"${METAFLOW_INIT_SCRIPT}\"} && true && mkdir -p $PWD/.logs && export PYTHONUNBUFFERED=x MF_PATHSPEC=HelloFlow/1669212389901135/hello/2 MF_DATASTORE=s3 MF_ATTEMPT=0 MFLOG_STDOUT=$PWD/.logs/mflog_stdout MFLOG_STDERR=$PWD/.logs/mflog_stderr && mflog(){ T=$(date -u -Ins|tr , .); echo \"[MFLOG|0|${T:0:26}Z|task|$T]$1\" >> $MFLOG_STDOUT; echo $1;  } && mflog \\'Setting up task environment.\\' && python -m pip install requests -qqq && python -m pip install awscli boto3 -qqq && mkdir metaflow && cd metaflow && mkdir .metaflow && i=0; while [ $i -le 5 ]; do mflog \\'Downloading code package...\\'; python -m awscli ${METAFLOW_S3_ENDPOINT_URL:+--endpoint-url=\"${METAFLOW_S3_ENDPOINT_URL}\"} s3 cp s3://mflowtest/HelloFlow/data/52/522f1c653a18e07a17a374b59a9df4fa72985897 job.tar >/dev/null && mflog \\'Code package downloaded.\\' && break; sleep 10; i=$((i+1)); done && if [ $i -gt 5 ]; then mflog \\'Failed to download code package from s3://mflowtest/HelloFlow/data/52/522f1c653a18e07a17a374b59a9df4fa72985897 after 6 tries. Exiting...\\' && exit 1; fi && TAR_OPTIONS=\\'--warning=no-timestamp\\' tar xf job.tar && mflog \\'Task is starting.\\' && (python -u helloworld.py --quiet --metadata local --environment local --datastore s3 --event-logger nullSidecarLogger --monitor nullSidecarMonitor --datastore-root s3://mflowtest --package-suffixes .py,.R,.RDS --pylint step hello --run-id 1669212389901135 --task-id 2 --input-paths ${METAFLOW_INPUT_PATHS_0} --retry-count 0 --max-user-code-retries 0 --namespace user:timpefr) 1>> >(python -m metaflow.mflog.tee task $MFLOG_STDOUT) 2>> >(python -m metaflow.mflog.tee task $MFLOG_STDERR >&2); c=$?; python -m metaflow.mflog.save_logs; exit $c']"

#
# Generated by a foreach flow with @nuvolaris on at least 1 step
#
['bash', '-c', '${METAFLOW_INIT_SCRIPT:+eval \"${METAFLOW_INIT_SCRIPT}\"} && true && mkdir -p $PWD/.logs && export PYTHONUNBUFFERED=x MF_PATHSPEC=ForeachFlow/1669560258054415/a/2 MF_DATASTORE=s3 MF_ATTEMPT=0 MFLOG_STDOUT=$PWD/.logs/mflog_stdout MFLOG_STDERR=$PWD/.logs/mflog_stderr && mflog(){ T=$(date -u -Ins|tr , .); echo \"[MFLOG|0|${T:0:26}Z|task|$T]$1\" >> $MFLOG_STDOUT; echo $1;  } && mflog \\'Setting up task environment.\\' && mkdir metaflow && cd metaflow && mkdir .metaflow && i=0; while [ $i -le 5 ]; do mflog \\'Downloading code package...\\'; python -m awscli ${METAFLOW_S3_ENDPOINT_URL:+--endpoint-url=\"${METAFLOW_S3_ENDPOINT_URL}\"} s3 cp s3://mflowtest/ForeachFlow/data/82/82232d5bd20cdc8f9226ca01dd196325f5dbaa82 job.tar >/dev/null && mflog \\'Code package downloaded.\\' && break; sleep 10; i=$((i+1)); done && if [ $i -gt 5 ]; then mflog \\'Failed to download code package from s3://mflowtest/ForeachFlow/data/82/82232d5bd20cdc8f9226ca01dd196325f5dbaa82 after 6 tries. Exiting...\\' && exit 1; fi && TAR_OPTIONS=\\'--warning=no-timestamp\\' tar xf job.tar && mflog \\'Task is starting.\\' && (python -u helloworld3.py --quiet --metadata local --environment local --datastore s3 --event-logger nullSidecarLogger --monitor nullSidecarMonitor --datastore-root s3://mflowtest --package-suffixes .py,.R,.RDS --pylint step a --run-id 1669560258054415 --task-id 2 --input-paths ${METAFLOW_INPUT_PATHS_0} --split-index 0 --retry-count 0 --max-user-code-retries 0 --namespace user:timpefr) 1>> >(python -m metaflow.mflog.tee task $MFLOG_STDOUT) 2>> >(python -m metaflow.mflog.tee task $MFLOG_STDERR >&2); c=$?; python -m metaflow.mflog.save_logs; exit $c']"

['bash', '-c', '${METAFLOW_INIT_SCRIPT:+eval \"${METAFLOW_INIT_SCRIPT}\"} && true && mkdir -p $PWD/.logs && export PYTHONUNBUFFERED=x MF_PATHSPEC=BranchFlow/1669560505080918/a/2 MF_DATASTORE=s3 MF_ATTEMPT=0 MFLOG_STDOUT=$PWD/.logs/mflog_stdout MFLOG_STDERR=$PWD/.logs/mflog_stderr && mflog(){ T=$(date -u -Ins|tr , .); echo \"[MFLOG|0|${T:0:26}Z|task|$T]$1\" >> $MFLOG_STDOUT; echo $1;  } && mflog \\'Setting up task environment.\\' && mkdir metaflow && cd metaflow && mkdir .metaflow && i=0; while [ $i -le 5 ]; do mflog \\'Downloading code package...\\'; python -m awscli ${METAFLOW_S3_ENDPOINT_URL:+--endpoint-url=\"${METAFLOW_S3_ENDPOINT_URL}\"} s3 cp s3://mflowtest/BranchFlow/data/58/58f98c16c13438cf2bbd118c7198f78067cf05c0 job.tar >/dev/null && mflog \\'Code package downloaded.\\' && break; sleep 10; i=$((i+1)); done && if [ $i -gt 5 ]; then mflog \\'Failed to download code package from s3://mflowtest/BranchFlow/data/58/58f98c16c13438cf2bbd118c7198f78067cf05c0 after 6 tries. Exiting...\\' && exit 1; fi && TAR_OPTIONS=\\'--warning=no-timestamp\\' tar xf job.tar && mflog \\'Task is starting.\\' && (python -u helloworld2.py --quiet --metadata local --environment local --datastore s3 --event-logger nullSidecarLogger --monitor nullSidecarMonitor --datastore-root s3://mflowtest --package-suffixes .py,.R,.RDS --pylint step a --run-id 1669560505080918 --task-id 2 --input-paths ${METAFLOW_INPUT_PATHS_0} --retry-count 0 --max-user-code-retries 0 --namespace user:timpefr) 1>> >(python -m metaflow.mflog.tee task $MFLOG_STDOUT) 2>> >(python -m metaflow.mflog.tee task $MFLOG_STDERR >&2); c=$?; python -m metaflow.mflog.save_logs; exit $c']"

debug[s3client /action/1/bin/metaflow/metaflow/datatools/s3.py:1359]:  /action/1/bin/metaflow/metaflow/datatools/s3op.py put --filelist /action/1/bin/metaflow/metaflow.s3.m0xe3w8f/metaflow.s3.put_inputs.79tt4wt8 --no-verbose --overwrite --listing